# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
The dataset represents direct markting campaign of a bank. Each record represnts a potential target for deposit placement with the bank and recorded results whether effort led to placement of deposit or not (target column "y"). 
Task of machine learning was to train a binary classification model so as to predict whether a potential target of marketing campaign will place deposit or not. 

We used two machine learning pipelines, with primary metric as "Accuracy", as follws: 

i) Scikit-learn piple line with Logistic Regrsssion Classifier and hyperdrive for hyperparameter tuning.

ii) Microsoft Azure proprietary AutoML pipeline for automated machine learing including selection of classifiers and their hyperparameters.

Best models selected under each of the pipelines are as follows: 

i) Scikit-learn : Logistic Regression model with 91.138% Accuracy  and with parameter values: C1: 0.48392209 , max_iter: 200.

ii) Azure AutoML:   VotingEnsemble classifier  with 91.639 % Accuracy. Some of parameter values:  n_estimators: 200,  min_samples_lea = 0.03578                 

Therefore best pipeline was AzureAutoML and best model was VotingEnsemble classifier with 91.639 % Accuracy

## Scikit-learn Pipeline

**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

Scikit-learn pipeline was based on prescribed LogisticRegression classifier. This was suitable as task was binary classification (yes,no). 

Data comprised of 32,950 records. There were 29258 records of "No" class and 3692 records of "Yes" class. Thus data has class imbalance issue.  It was split into training and test set with 80/20 split. The 

Two hyperparameters tuned are

i) C : Inverse of regularization stregth (smaller value stronger regularization)

ii) max_iter: Maximum number of iterations taken for the solvers to converge.

We used Hyperdrive package for hyperparameter tunning. This helped to speed model training as we didnot had to manually experiment with hyperparamete tuning. 

In order to define sample space for hyperparamter tuning we  used RandomParameterSampling class. In this sampling algorithm, parameter values are chosen from a set of discrete values or a distribution over a continuous range.

**What are the benefits of the parameter sampler you chose?**

For Hyperdrive package other sampling class available are 

i) GridParameter Sampling: Defines a grid of samples over hyperparameter search space.

ii) BayesianParameter Sampling: It tries to intelligently pick the next sample of hyperparameters based on how the previous sample performed. 

As compared to GridParameter sampling Random Parameter is much faster as it is not required to search over entire grid of sample space. However as compared to Bayesian method over selected apprach lacks the element of learning from prior sample performance.

**What are the benefits of the early stopping policy you chose?**

Early stopping policies are useful to stop as run when specified policy criteria are met.

We used Bandit policy that defines early termination based on slack criteria and frequency and delay interval for evaluation. Any run that doesn't fall within the slack factor or slack amount of the evaluation metric with respect to the best performing run will be terminated.

Other available stopping policy is MedianStoppingPolicy. The Median Stopping policy computes running averages across all runs and cancels runs whose best performance is worse than the median of the running averages.

Use of termination policy like Bandit is more time and resource efficient as compared to NoTerminationPolicy option, as it ensures that all runs that are certain% worst then the best current run are killed. 


## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
AutoML pipeline evaluated 26 classifiers. These range from simple classifiers such as Logistic Regression and Random forest to complex ensemble classifiers such XGBoostClassiers, GradientBoosting etc. It was able to tune across a large no of hyperparemeters (10 main for best classifier)

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
We used AmlCompute command delete() (documented in our python notebook) to clean up cluster when both pipeline runs has finished.
